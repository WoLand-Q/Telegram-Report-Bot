import logging
import sys
import os
import glob
import datetime
import requests
import openpyxl
import json
from datetime import time
import pytz
from typing import Dict, List, Any, Tuple
from telegram import Update, InlineKeyboardMarkup, InlineKeyboardButton
from telegram.ext import (
    ApplicationBuilder,
    CommandHandler,
    MessageHandler,
    CallbackQueryHandler,
    ConversationHandler,
    filters,
    ContextTypes,
)

# ----------------- –õ–û–ì–ò–†–û–í–ê–ù–ò–ï -----------------
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)

# ----------------- –ù–ê–°–¢–†–û–ô–ö–ò -----------------
BOT_TOKEN = ""  # –í–∞—à —Ç–æ–∫–µ–Ω
ADMIN_CHAT_ID =  43545 # ID —á–∞—Ç–∞ –¥–ª—è –æ—Ç—á—ë—Ç–æ–≤

IIKO_HOST = ""
LOGIN = ""
PASSWORD_SHA1 = ""  # SHA1

PLAN_FACT_FOLDER = "data_excels"

# –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
CATEGORIES = ["–¥–æ—Å—Ç–∞–≤–∫–∞", "–∑–∞–ª", "–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã"]

REPORT_TYPE = "SALES"
GROUP_BY_ROW_FIELDS = ["Department", "OrderType"]
AGGREGATE_FIELDS = [
    "GuestNum",
    "UniqOrderId.OrdersCount",
    "DishDiscountSumInt",
    "DishDiscountSumInt.average"
]
BUILD_SUMMARY = True

FILTERS = {
    "OpenDate.Typed": {
        "filterType": "DateRange",
        "periodType": "CUSTOM",
        "from": None,
        "to": None,
        "includeLow": True,
        "includeHigh": False
    },
    "DeletedWithWriteoff": {"filterType": "IncludeValues", "values": ["NOT_DELETED"]},
    "OrderDeleted": {"filterType": "IncludeValues", "values": ["NOT_DELETED"]}
}

# ----- –ù–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞–≤—Ç–æ–æ—Ç—á—ë—Ç–∞ -----
# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ—Ç–∏: –∫–ª—é—á ‚Äì –∏–º—è —Å–µ—Ç–∏, –∑–Ω–∞—á–µ–Ω–∏–µ ‚Äì —Å–ø–∏—Å–æ–∫ —Ç–æ—á–µ–∫ (–∏–º—ë–Ω —Ñ–∞–π–ª–æ–≤ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
NETWORK_GROUPS = {
    "–ö–∏–µ–≤": [""],
    "–î–Ω–µ–ø—Ä": ["", "", ""],
    "–•–∞—Ä—å–∫–æ–≤": ["", "", ""]
}
# –§–∞–π–ª —Å Telegram ID –¥–ª—è –∞–≤—Ç–æ–æ—Ç—á—ë—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, [123456789, 987654321])
AUTO_REPORT_USERS_FILE = "auto_report_users.json"


# ----------- –§—É–Ω–∫—Ü–∏—è —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è Markdown -----------
def escape_markdown(text: str) -> str:
    """
    –≠–∫—Ä–∞–Ω–∏—Ä—É–µ—Ç —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã Markdown (–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π Markdown) ‚Äì *, _, `, [.
    """
    text = text.replace("\\", "\\\\")
    escape_chars = r"*_[`["
    for char in escape_chars:
        text = text.replace(char, f"\\{char}")
    return text


# ----------------- –§–£–ù–ö–¶–ò–ò –î–õ–Ø IIKO -----------------
def map_order_type_to_category(order_type_str: str) -> str:
    if not order_type_str:
        return "–∑–∞–ª"
    lower_ot = order_type_str.lower()
    if any(x in lower_ot for x in ["bolt", "glovo", "delivery hub", "–ø—é—Ä–µ—à–∫–∞ & –∫–æ—Ç–ª–µ—Ç–∫–∞"]):
        return "–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã"
    return "–¥–æ—Å—Ç–∞–≤–∫–∞"


def safe_float(cell_value) -> float:
    if cell_value is None:
        return 0.0
    try:
        val_str = str(cell_value).replace('\xa0', '').strip()
        return float(val_str) if val_str else 0.0
    except ValueError:
        return 0.0


def iiko_login(session: requests.Session) -> str:
    auth_url = f"{IIKO_HOST}/resto/api/auth"
    payload = {"login": LOGIN, "pass": PASSWORD_SHA1}
    try:
        resp = session.post(auth_url, data=payload, timeout=10)
        resp.raise_for_status()
        token = resp.text.strip()
        logging.info("–£—Å–ø–µ—à–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ iiko. –¢–æ–∫–µ–Ω: %s", token)
        return token
    except requests.exceptions.RequestException as exc:
        logging.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ iiko: %s", exc)
        sys.exit(1)


def iiko_logout(session: requests.Session, token: str):
    logout_url = f"{IIKO_HOST}/resto/api/logout"
    payload = {"key": token}
    try:
        resp = session.post(logout_url, data=payload, timeout=10)
        if resp.status_code == 200:
            logging.info("–£—Å–ø–µ—à–Ω—ã–π logout –≤ iiko.")
        else:
            logging.warning("Logout –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å %s", resp.status_code)
    except requests.exceptions.RequestException as exc:
        logging.warning("–û—à–∏–±–∫–∞ –ø—Ä–∏ logout iiko: %s", exc)


def build_olap_request_body(filters: dict) -> dict:
    return {
        "reportType": REPORT_TYPE,
        "buildSummary": BUILD_SUMMARY,
        "groupByRowFields": GROUP_BY_ROW_FIELDS,
        "aggregateFields": AGGREGATE_FIELDS,
        "filters": filters
    }


def fetch_olap_report(session: requests.Session, token: str, body: dict) -> dict:
    olap_url = f"{IIKO_HOST}/resto/api/v2/reports/olap"
    headers = {"Content-Type": "application/json; charset=utf-8"}
    try:
        resp = session.post(
            olap_url,
            params={"key": token},
            headers=headers,
            json=body,
            timeout=30
        )
        resp.raise_for_status()
        data = resp.json()
        logging.info("OLAP –æ—Ç–≤–µ—Ç:\n%s", json.dumps(data, ensure_ascii=False, indent=4))
        return data
    except requests.exceptions.RequestException as exc:
        logging.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ OLAP: %s", exc)
        sys.exit(1)


def get_report_for_department(session: requests.Session, token: str,
                              department_name: str, date_from: str, date_to: str) -> list:
    filters_updated = FILTERS.copy()
    filters_updated["OpenDate.Typed"]["from"] = f"{date_from}T00:00:00.000"
    filters_updated["OpenDate.Typed"]["to"] = f"{date_to}T00:00:00.000"
    filters_updated["Department"] = {"filterType": "IncludeValues", "values": [department_name]}
    body = build_olap_request_body(filters_updated)
    res_json = fetch_olap_report(session, token, body)
    data_rows = res_json.get("data", [])
    logging.info("–î–ª—è –∑–∞–≤–µ–¥–µ–Ω–∏—è '%s' –ø–æ–ª—É—á–µ–Ω–æ %d —Å—Ç—Ä–æ–∫ –∏–∑ OLAP.", department_name, len(data_rows))
    return data_rows


# ---------------- –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–ª–∞–Ω/—Ñ–∞–∫—Ç –∏–∑ Excel ----------------
def parse_plan_fact_excel(file_path: str) -> Dict[Tuple[str, str], Dict[str, float]]:
    wb = openpyxl.load_workbook(file_path)
    sheet = wb.active
    plan_fact_data = {}
    for row in range(2, sheet.max_row + 1):
        raw_date = sheet.cell(row=row, column=2).value
        if not raw_date:
            continue
        try:
            dt = datetime.datetime.strptime(str(raw_date).strip(), "%d.%m.%Y")
            date_key = dt.date().isoformat()
        except Exception as e:
            logging.error("–û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞—Ç—ã '%s': %s", raw_date, e)
            continue

        plan_total_sales = safe_float(sheet.cell(row=row, column=3).value)
        plan_sales_hall = safe_float(sheet.cell(row=row, column=4).value)
        plan_sales_deliv = safe_float(sheet.cell(row=row, column=5).value)
        plan_sales_agg = safe_float(sheet.cell(row=row, column=6).value)
        plan_avg_check_hall = safe_float(sheet.cell(row=row, column=7).value)
        plan_avg_guest_hall = safe_float(sheet.cell(row=row, column=8).value)  # –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        plan_guests_hall = safe_float(sheet.cell(row=row, column=9).value)  # "–ì–æ—Å—Ç–∏ –ó–∞–ª"
        plan_orders_hall = safe_float(sheet.cell(row=row, column=10).value)
        plan_avg_check_deliv = safe_float(sheet.cell(row=row, column=11).value)
        plan_avg_check_agg = safe_float(sheet.cell(row=row, column=12).value)
        plan_orders_deliv = safe_float(sheet.cell(row=row, column=13).value)
        plan_orders_agg = safe_float(sheet.cell(row=row, column=14).value)

        plan_fact_data[(date_key, "–∏—Ç–æ–≥–æ")] = {
            "plan_total_sales": plan_total_sales
        }
        plan_fact_data[(date_key, "–∑–∞–ª")] = {
            "plan_sales": plan_sales_hall,
            "plan_orders": plan_orders_hall,
            "plan_avg_check": plan_avg_check_hall,
            "plan_guests": plan_guests_hall
        }
        plan_fact_data[(date_key, "–¥–æ—Å—Ç–∞–≤–∫–∞")] = {
            "plan_sales": plan_sales_deliv,
            "plan_orders": plan_orders_deliv,
            "plan_avg_check": plan_avg_check_deliv,
        }
        plan_fact_data[(date_key, "–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã")] = {
            "plan_sales": plan_sales_agg,
            "plan_orders": plan_orders_agg,
            "plan_avg_check": plan_avg_check_agg,
        }
    return plan_fact_data


def combine_plan_fact_with_iiko(
        plan_fact_data: Dict[Tuple[str, str], Dict[str, float]],
        iiko_data: List[Dict[str, Any]],
        target_date: str,
        category: str
) -> Dict[str, float]:
    key = (target_date, category.lower())
    pf_values = plan_fact_data.get(key, {})

    fact_sum = 0.0
    fact_orders = 0.0
    fact_guests = 0.0

    for row in iiko_data:
        order_type_raw = (row.get("OrderType") or "").strip()
        cat_mapped = map_order_type_to_category(order_type_raw)
        if cat_mapped == category.lower():
            fact_sum += float(row.get("DishDiscountSumInt", 0.0))
            fact_orders += float(row.get("UniqOrderId.OrdersCount", 0.0))
            if category.lower() == "–∑–∞–ª":
                fact_guests += float(row.get("GuestNum", 0.0))

    combined = {
        "plan_sales": pf_values.get("plan_sales", 0.0),
        "plan_orders": pf_values.get("plan_orders", 0.0),
        "plan_avg_check": pf_values.get("plan_avg_check", 0.0),
        "fact_sales": fact_sum,
        "fact_orders": fact_orders,
        "fact_avg_check": fact_sum / fact_orders if fact_orders else 0.0
    }
    if category.lower() == "–∑–∞–ª":
        combined["plan_guests"] = pf_values.get("plan_guests", 0.0)
        combined["fact_guests"] = fact_guests
    return combined


def get_detailed_plan_fact(department: str, target_date: str) -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø–ª–∞–Ω/—Ñ–∞–∫—Ç –¥–ª—è –∑–∞–≤–µ–¥–µ–Ω–∏—è (–∏–º—è —Ñ–∞–π–ª–∞ = –Ω–∞–∑–≤–∞–Ω–∏–µ –∑–∞–≤–µ–¥–µ–Ω–∏—è)
    –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—É—é –¥–∞—Ç—É (—Ñ–æ—Ä–º–∞—Ç YYYY-MM-DD) —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –æ–±—â–µ–π —Å–≤–æ–¥–∫–æ–π.
    """
    file_path = os.path.join(PLAN_FACT_FOLDER, f"{department}.xlsx")
    if not os.path.exists(file_path):
        logging.error("–§–∞–π–ª –¥–ª—è –∑–∞–≤–µ–¥–µ–Ω–∏—è '%s' –Ω–µ –Ω–∞–π–¥–µ–Ω.", department)
        return {}
    pf_data = parse_plan_fact_excel(file_path)
    date_from = target_date
    date_to = (datetime.datetime.strptime(target_date, "%Y-%m-%d") + datetime.timedelta(days=1)).date().isoformat()

    with requests.Session() as session:
        token = iiko_login(session)
        iiko_data = get_report_for_department(session, token, department, date_from, date_to)
        iiko_logout(session, token)

    details = {}
    overall_fact_sales = 0.0
    overall_fact_orders = 0.0
    overall_plan_orders = 0.0
    overall_fact_guests = 0.0

    for cat in CATEGORIES:
        res = combine_plan_fact_with_iiko(pf_data, iiko_data, target_date, cat)
        details[cat] = res
        overall_fact_sales += res["fact_sales"]
        overall_fact_orders += res["fact_orders"]
        overall_plan_orders += res["plan_orders"]
        if cat.lower() == "–∑–∞–ª":
            overall_fact_guests += res.get("fact_guests", 0.0)

    overall_plan = pf_data.get((target_date, "–∏—Ç–æ–≥–æ"), {}).get("plan_total_sales", 0.0)
    overall_plan_avg = overall_plan / overall_plan_orders if overall_plan_orders else 0.0
    overall_fact_avg = overall_fact_sales / overall_fact_orders if overall_fact_orders else 0.0

    overall = {
        "plan_total_sales": overall_plan,
        "plan_orders": overall_plan_orders,
        "fact_sales": overall_fact_sales,
        "fact_orders": overall_fact_orders,
        "plan_avg_check": overall_plan_avg,
        "fact_avg_check": overall_fact_avg,
        "plan_guests": pf_data.get((target_date, "–∑–∞–ª"), {}).get("plan_guests", 0.0),
        "fact_guests": overall_fact_guests
    }

    return {
        "department": department,
        "target_date": target_date,
        "details": details,
        "overall": overall
    }


# ----------------- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è -----------------
async def send_long_message(context: ContextTypes.DEFAULT_TYPE, chat_id: int, text: str, max_length: int = 3500):
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ —á–∞—Å—Ç—è–º–∏, —Ä–∞–∑–±–∏–≤–∞—è –ø–æ —Å—Ç—Ä–æ–∫–∞–º, —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—Ç—å Markdown-—Å—É—â–Ω–æ—Å—Ç–∏.
    """
    lines = text.split("\n")
    chunk = ""
    for line in lines:
        if len(chunk) + len(line) + 1 > max_length:
            await context.bot.send_message(chat_id=chat_id, text=chunk, parse_mode="Markdown")
            chunk = line
        else:
            if chunk:
                chunk += "\n" + line
            else:
                chunk = line
    if chunk:
        await context.bot.send_message(chat_id=chat_id, text=chunk, parse_mode="Markdown")


# ----------------- –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å /get_plan_fact —á–µ—Ä–µ–∑ ConversationHandler -----------------
GET_DATE, CHOOSE_DEPARTMENT = range(2)


async def get_plan_fact_start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–í–≤–µ–¥–∏—Ç–µ –¥–∞—Ç—É (–≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD) –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–ª–∞–Ω–∞/—Ñ–∞–∫—Ç–∞:")
    return GET_DATE


async def get_date_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = update.message.text.strip()
    try:
        dt = datetime.datetime.strptime(text, "%Y-%m-%d").date()
    except ValueError:
        await update.message.reply_text("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD:")
        return GET_DATE
    context.user_data["target_date"] = dt.isoformat()

    # –°–ø–∏—Å–æ–∫ –∑–∞–≤–µ–¥–µ–Ω–∏–π ‚Äî –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    files = glob.glob(os.path.join(PLAN_FACT_FOLDER, "*.xlsx"))
    if not files:
        await update.message.reply_text("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞–≤–µ–¥–µ–Ω–∏–π.")
        return ConversationHandler.END
    departments = [os.path.splitext(os.path.basename(f))[0] for f in files]

    keyboard = [[InlineKeyboardButton(dept, callback_data=dept)] for dept in departments]
    reply_markup = InlineKeyboardMarkup(keyboard)
    await update.message.reply_text("–í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–≤–µ–¥–µ–Ω–∏–µ:", reply_markup=reply_markup)
    return CHOOSE_DEPARTMENT


async def choose_department_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    department = query.data
    target_date = context.user_data.get("target_date")
    if not target_date:
        await query.edit_message_text("–û—à–∏–±–∫–∞: –Ω–µ –∑–∞–¥–∞–Ω–∞ –¥–∞—Ç–∞.")
        return ConversationHandler.END

    data = get_detailed_plan_fact(department, target_date)
    if not data:
        await query.edit_message_text("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.")
        return ConversationHandler.END

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –æ–±—â–µ–π —Å–≤–æ–¥–∫–æ–π
    emoji_map = {"–¥–æ—Å—Ç–∞–≤–∫–∞": "üöö", "–∑–∞–ª": "üè∞", "–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã": "üì¶"}
    lines = []
    lines.append(f"üè¢ –ó–∞–≤–µ–¥–µ–Ω–∏–µ: {escape_markdown(department)}")
    lines.append("---\n")
    details = data["details"]
    for cat in CATEGORIES:
        cat_data = details.get(cat, {})
        emoji = emoji_map.get(cat.lower(), "‚Ä¢")
        cat_title = cat.capitalize()
        lines.append(f"{emoji} {escape_markdown(cat_title)}:")
        lines.append(
            f"‚Ä¢ *–ü–ª–∞–Ω –ü—Ä–æ–¥–∞–∂–∏:* {cat_data.get('plan_sales', 0):.0f} –≥—Ä–Ω | *–§–∞–∫—Ç –ü—Ä–æ–¥–∞–∂–∏:* {cat_data.get('fact_sales', 0):.0f} –≥—Ä–Ω")
        lines.append(
            f"‚Ä¢ *–ü–ª–∞–Ω –ó–∞–∫–∞–∑–æ–≤:* {cat_data.get('plan_orders', 0):.0f} | *–§–∞–∫—Ç –ó–∞–∫–∞–∑–æ–≤:* {cat_data.get('fact_orders', 0):.0f}")
        lines.append(
            f"‚Ä¢ *–ü–ª–∞–Ω –°—Ä.–ó–∞–∫–∞–∑:* {cat_data.get('plan_avg_check', 0):.0f} –≥—Ä–Ω | *–§–∞–∫—Ç –°—Ä.–ó–∞–∫–∞–∑:* {cat_data.get('fact_avg_check', 0):.2f} –≥—Ä–Ω")
        if cat.lower() == "–∑–∞–ª":
            lines.append(
                f"‚Ä¢ *–ü–ª–∞–Ω –ì–æ—Å—Ç–µ–π –ó–∞–ª:* {cat_data.get('plan_guests', 0):.0f} | *–§–∞–∫—Ç –ì–æ—Å—Ç–µ–π:* {cat_data.get('fact_guests', 0):.0f}")
        lines.append("")
    overall = data["overall"]
    lines.append("üè∑Ô∏è –û–±—â–∞—è (–¥–æ—Å—Ç–∞–≤–∫–∞+–∑–∞–ª+–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã):")
    lines.append(
        f"‚Ä¢ *–ü–ª–∞–Ω –ü—Ä–æ–¥–∞–∂–∏ (–ò—Ç–æ–≥–æ):* {overall.get('plan_total_sales', 0):.0f} –≥—Ä–Ω | *–§–∞–∫—Ç –ü—Ä–æ–¥–∞–∂–∏:* {overall.get('fact_sales', 0):.0f} –≥—Ä–Ω")
    lines.append(
        f"‚Ä¢ *–ü–ª–∞–Ω –ó–∞–∫–∞–∑–æ–≤ (—Å—É–º–º.):* {overall.get('plan_orders', 0):.0f} | *–§–∞–∫—Ç –ó–∞–∫–∞–∑–æ–≤:* {overall.get('fact_orders', 0):.0f}")
    lines.append(
        f"‚Ä¢ *–ü–ª–∞–Ω –°—Ä.–ó–∞–∫–∞–∑:* {overall.get('plan_avg_check', 0):.0f} –≥—Ä–Ω | *–§–∞–∫—Ç –°—Ä.–ó–∞–∫–∞–∑:* {overall.get('fact_avg_check', 0):.2f} –≥—Ä–Ω")
    lines.append(
        f"‚Ä¢ *–ü–ª–∞–Ω –ì–æ—Å—Ç–µ–π (–∑–∞–ª):* {overall.get('plan_guests', 0):.0f} | *–§–∞–∫—Ç –ì–æ—Å—Ç–µ–π:* {overall.get('fact_guests', 0):.0f}")
    final_text = "\n".join(lines)
    await query.edit_message_text(final_text, parse_mode="Markdown")
    return ConversationHandler.END


async def cancel_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞.")
    return ConversationHandler.END


# ----------------- –ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≤—Ç–æ–æ—Ç—á—ë—Ç –ø–æ —Å–µ—Ç—è–º -----------------
def get_aggregated_network_plan_fact(target_date: str) -> Dict[str, Any]:
    """
    –î–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π –¥–∞—Ç—ã (YYYY-MM-DD) –ø–æ–ª—É—á–∞–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Å–µ–º —Ç–æ—á–∫–∞–º, –≤—Ö–æ–¥—è—â–∏–º –≤ —Å–µ—Ç–∏,
    –∑–∞–¥–∞–Ω–Ω—ã–µ –≤ NETWORK_GROUPS. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –æ–±—â—É—é —Å–≤–æ–¥–∫—É.
    """
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    agg_categories = {cat: {"plan_sales": 0.0, "fact_sales": 0.0,
                            "plan_orders": 0.0, "fact_orders": 0.0} for cat in CATEGORIES}
    # –î–ª—è –∑–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º –≥–æ—Å—Ç–µ–π
    agg_categories["–∑–∞–ª"].update({"plan_guests": 0.0, "fact_guests": 0.0})
    # –û–±—â–∏–µ –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä—ã
    overall = {"plan_total_sales": 0.0, "plan_orders": 0.0,
               "fact_sales": 0.0, "fact_orders": 0.0,
               "plan_guests": 0.0, "fact_guests": 0.0}

    # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º —Å–µ—Ç–∏
    for network, departments in NETWORK_GROUPS.items():
        for dept in departments:
            file_path = os.path.join(PLAN_FACT_FOLDER, f"{dept}.xlsx")
            if not os.path.exists(file_path):
                logging.warning("–§–∞–π–ª –¥–ª—è —Ç–æ—á–∫–∏ '%s' –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.", dept)
                continue
            data = get_detailed_plan_fact(dept, target_date)
            if not data:
                continue
            # –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            details = data.get("details", {})
            for cat in CATEGORIES:
                cat_data = details.get(cat, {})
                agg_categories[cat]["plan_sales"] += cat_data.get("plan_sales", 0)
                agg_categories[cat]["fact_sales"] += cat_data.get("fact_sales", 0)
                agg_categories[cat]["plan_orders"] += cat_data.get("plan_orders", 0)
                agg_categories[cat]["fact_orders"] += cat_data.get("fact_orders", 0)
                if cat.lower() == "–∑–∞–ª":
                    agg_categories[cat]["plan_guests"] += cat_data.get("plan_guests", 0)
                    agg_categories[cat]["fact_guests"] += cat_data.get("fact_guests", 0)
            # –û–±—â–∞—è —Å–≤–æ–¥–∫–∞
            overall_data = data.get("overall", {})
            overall["plan_total_sales"] += overall_data.get("plan_total_sales", 0)
            overall["plan_orders"] += overall_data.get("plan_orders", 0)
            overall["fact_sales"] += overall_data.get("fact_sales", 0)
            overall["fact_orders"] += overall_data.get("fact_orders", 0)
            overall["plan_guests"] += overall_data.get("plan_guests", 0)
            overall["fact_guests"] += overall_data.get("fact_guests", 0)

    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ (—Å—Ä–µ–¥–Ω–∏–π —á–µ–∫) –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    for cat in CATEGORIES:
        cat_dict = agg_categories[cat]
        if cat_dict["plan_orders"]:
            cat_dict["plan_avg_check"] = cat_dict["plan_sales"] / cat_dict["plan_orders"]
        else:
            cat_dict["plan_avg_check"] = 0.0
        if cat_dict["fact_orders"]:
            cat_dict["fact_avg_check"] = cat_dict["fact_sales"] / cat_dict["fact_orders"]
        else:
            cat_dict["fact_avg_check"] = 0.0

    # –û–±—â–∏–π —Å—Ä–µ–¥–Ω–∏–π —á–µ–∫ (—Å—É–º–º–∏—Ä—É–µ–º –ø–æ –≤—Å–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º)
    total_plan_orders = sum(agg_categories[cat]["plan_orders"] for cat in CATEGORIES)
    total_fact_orders = sum(agg_categories[cat]["fact_orders"] for cat in CATEGORIES)
    if total_plan_orders:
        overall["plan_avg_check"] = sum(agg_categories[cat]["plan_sales"] for cat in CATEGORIES) / total_plan_orders
    else:
        overall["plan_avg_check"] = 0.0
    if total_fact_orders:
        overall["fact_avg_check"] = sum(agg_categories[cat]["fact_sales"] for cat in CATEGORIES) / total_fact_orders
    else:
        overall["fact_avg_check"] = 0.0

    return {
        "networks": list(NETWORK_GROUPS.keys()),
        "categories": agg_categories,
        "overall": overall
    }


async def auto_report_job(context: ContextTypes.DEFAULT_TYPE):
    """
    –ï–∂–µ–¥–Ω–µ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞, –∫–æ—Ç–æ—Ä–∞—è –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∞–≤—Ç–æ–æ—Ç—á—ë—Ç –∑–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–π –¥–µ–Ω—å (–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ –≤—Å–µ–º —Å–µ—Ç—è–º)
    –Ω–∞ —Å–ø–∏—Å–æ–∫ Telegram-ID, —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª–µ AUTO_REPORT_USERS_FILE.
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∞—Ç—É –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –¥–Ω—è
    yesterday = datetime.date.today() - datetime.timedelta(days=1)
    target_date = yesterday.isoformat()

    # –ü–æ–ª—É—á–∞–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    agg_data = get_aggregated_network_plan_fact(target_date)
    networks = ", ".join(agg_data["networks"])
    cat_data = agg_data["categories"]
    overall = agg_data["overall"]

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –æ—Ç—á—ë—Ç–∞
    lines = []
    lines.append(f"*–ê–≤—Ç–æ–æ—Ç—á—ë—Ç –∑–∞ {escape_markdown(target_date)}*")
    lines.append(f"\n–°–µ—Ç—å: {escape_markdown(networks)}")
    lines.append("---\n")

    # –î–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    emoji_map = {"–¥–æ—Å—Ç–∞–≤–∫–∞": "üöö", "–∑–∞–ª": "üè∞", "–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã": "üì¶"}
    for cat in CATEGORIES:
        data_cat = cat_data.get(cat, {})
        emoji = emoji_map.get(cat.lower(), "‚Ä¢")
        cat_title = cat.capitalize()
        lines.append(f"{emoji} {escape_markdown(cat_title)}:")
        lines.append(
            f"‚Ä¢ *–ü–ª–∞–Ω –ü—Ä–æ–¥–∞–∂–∏:* {data_cat.get('plan_sales', 0):.0f} –≥—Ä–Ω | *–§–∞–∫—Ç –ü—Ä–æ–¥–∞–∂–∏:* {data_cat.get('fact_sales', 0):.0f} –≥—Ä–Ω")
        lines.append(
            f"‚Ä¢ *–ü–ª–∞–Ω –ó–∞–∫–∞–∑–æ–≤:* {data_cat.get('plan_orders', 0):.0f} | *–§–∞–∫—Ç –ó–∞–∫–∞–∑–æ–≤:* {data_cat.get('fact_orders', 0):.0f}")
        lines.append(
            f"‚Ä¢ *–ü–ª–∞–Ω –°—Ä.–ó–∞–∫–∞–∑:* {data_cat.get('plan_avg_check', 0):.0f} –≥—Ä–Ω | *–§–∞–∫—Ç –°—Ä.–ó–∞–∫–∞–∑:* {data_cat.get('fact_avg_check', 0):.2f} –≥—Ä–Ω")
        if cat.lower() == "–∑–∞–ª":
            lines.append(
                f"‚Ä¢ *–ü–ª–∞–Ω –ì–æ—Å—Ç–µ–π –ó–∞–ª:* {data_cat.get('plan_guests', 0):.0f} | *–§–∞–∫—Ç –ì–æ—Å—Ç–µ–π:* {data_cat.get('fact_guests', 0):.0f}")
        lines.append("")

    # –û–±—â–∞—è —Å–≤–æ–¥–∫–∞
    lines.append("üè∑Ô∏è –û–±—â–∞—è (–¥–æ—Å—Ç–∞–≤–∫–∞+–∑–∞–ª+–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã):")
    lines.append(
        f"‚Ä¢ *–ü–ª–∞–Ω –ü—Ä–æ–¥–∞–∂–∏ (–ò—Ç–æ–≥–æ):* {overall.get('plan_total_sales', 0):.0f} –≥—Ä–Ω | *–§–∞–∫—Ç –ü—Ä–æ–¥–∞–∂–∏:* {overall.get('fact_sales', 0):.0f} –≥—Ä–Ω")
    lines.append(
        f"‚Ä¢ *–ü–ª–∞–Ω –ó–∞–∫–∞–∑–æ–≤ (—Å—É–º–º.):* {overall.get('plan_orders', 0):.0f} | *–§–∞–∫—Ç –ó–∞–∫–∞–∑–æ–≤:* {overall.get('fact_orders', 0):.0f}")
    lines.append(
        f"‚Ä¢ *–ü–ª–∞–Ω –°—Ä.–ó–∞–∫–∞–∑:* {overall.get('plan_avg_check', 0):.0f} –≥—Ä–Ω | *–§–∞–∫—Ç –°—Ä.–ó–∞–∫–∞–∑:* {overall.get('fact_avg_check', 0):.2f} –≥—Ä–Ω")
    lines.append(
        f"‚Ä¢ *–ü–ª–∞–Ω –ì–æ—Å—Ç–µ–π (–∑–∞–ª):* {overall.get('plan_guests', 0):.0f} | *–§–∞–∫—Ç –ì–æ—Å—Ç–µ–π:* {overall.get('fact_guests', 0):.0f}")

    final_text = "\n".join(lines)

    # –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ Telegram ID –¥–ª—è –∞–≤—Ç–æ–æ—Ç—á—ë—Ç–æ–≤ –∏–∑ JSON-—Ñ–∞–π–ª–∞
    if os.path.exists(AUTO_REPORT_USERS_FILE):
        try:
            with open(AUTO_REPORT_USERS_FILE, "r", encoding="utf-8") as f:
                user_ids = json.load(f)
        except Exception as e:
            logging.error("–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ %s: %s", AUTO_REPORT_USERS_FILE, e)
            return
    else:
        logging.warning("–§–∞–π–ª %s –Ω–µ –Ω–∞–π–¥–µ–Ω. –ê–≤—Ç–æ–æ—Ç—á–µ—Ç –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω.", AUTO_REPORT_USERS_FILE)
        return

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç—á—ë—Ç –∫–∞–∂–¥–æ–º—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –∏–∑ —Å–ø–∏—Å–∫–∞
    for uid in user_ids:
        try:
            await context.bot.send_message(chat_id=uid, text=final_text, parse_mode="Markdown")
        except Exception as e:
            logging.error("–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞–≤—Ç–æ–æ—Ç—á—ë—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é %s: %s", uid, e)

    logging.info("–ê–≤—Ç–æ–æ—Ç—á—ë—Ç –∑–∞ %s —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω.", target_date)


# ----------------- –û—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –±–æ—Ç–∞ -----------------
async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞.")


async def upload_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–ü—Ä–∏—à–ª–∏—Ç–µ .xlsx-—Ñ–∞–π–ª ‚Äî –æ–Ω —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ –ø–∞–ø–∫—É data_excels.")


async def handle_document(update: Update, context: ContextTypes.DEFAULT_TYPE):
    document = update.message.document
    file_name = document.file_name
    if file_name.endswith(".xlsx"):
        file_path = os.path.join(PLAN_FACT_FOLDER, file_name)
        file_obj = await document.get_file()
        await file_obj.download_to_drive(file_path)
        await update.message.reply_text(f"–§–∞–π–ª '{file_name}' —Å–æ—Ö—Ä–∞–Ω—ë–Ω.")
    else:
        await update.message.reply_text("–≠—Ç–æ –Ω–µ .xlsx-—Ñ–∞–π–ª.")


async def test_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    # –î–ª—è —Ç–µ—Å—Ç–∞ –≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤–æ–µ –∑–∞–≤–µ–¥–µ–Ω–∏–µ –∏–∑ –ø–∞–ø–∫–∏
    files = glob.glob(os.path.join(PLAN_FACT_FOLDER, "*.xlsx"))
    if not files:
        await update.message.reply_text("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞.")
        return
    department = os.path.splitext(os.path.basename(files[0]))[0]
    target_date = datetime.date.today().isoformat()
    data = get_detailed_plan_fact(department, target_date)
    if not data:
        await update.message.reply_text("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∞.")
        return

    emoji_map = {"–¥–æ—Å—Ç–∞–≤–∫–∞": "üöö", "–∑–∞–ª": "üè∞", "–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã": "üì¶"}
    lines = []
    lines.append(f"üè¢ –ó–∞–≤–µ–¥–µ–Ω–∏–µ: {escape_markdown(department)}")
    lines.append("---\n")
    details = data["details"]
    for cat in CATEGORIES:
        cat_data = details.get(cat, {})
        emoji = emoji_map.get(cat.lower(), "‚Ä¢")
        cat_title = cat.capitalize()
        lines.append(f"{emoji} {escape_markdown(cat_title)}:")
        lines.append(
            f"‚Ä¢ *–ü–ª–∞–Ω –ü—Ä–æ–¥–∞–∂–∏:* {cat_data.get('plan_sales', 0):.0f} –≥—Ä–Ω | *–§–∞–∫—Ç –ü—Ä–æ–¥–∞–∂–∏:* {cat_data.get('fact_sales', 0):.0f} –≥—Ä–Ω")
        lines.append(
            f"‚Ä¢ *–ü–ª–∞–Ω –ó–∞–∫–∞–∑–æ–≤:* {cat_data.get('plan_orders', 0):.0f} | *–§–∞–∫—Ç –ó–∞–∫–∞–∑–æ–≤:* {cat_data.get('fact_orders', 0):.0f}")
        lines.append(
            f"‚Ä¢ *–ü–ª–∞–Ω –°—Ä.–ó–∞–∫–∞–∑:* {cat_data.get('plan_avg_check', 0):.0f} –≥—Ä–Ω | *–§–∞–∫—Ç –°—Ä.–ó–∞–∫–∞–∑:* {cat_data.get('fact_avg_check', 0):.2f} –≥—Ä–Ω")
        if cat.lower() == "–∑–∞–ª":
            lines.append(
                f"‚Ä¢ *–ü–ª–∞–Ω –ì–æ—Å—Ç–µ–π –ó–∞–ª:* {cat_data.get('plan_guests', 0):.0f} | *–§–∞–∫—Ç –ì–æ—Å—Ç–µ–π:* {cat_data.get('fact_guests', 0):.0f}")
        lines.append("")
    overall = data["overall"]
    lines.append("üè∑Ô∏è –û–±—â–∞—è (–¥–æ—Å—Ç–∞–≤–∫–∞+–∑–∞–ª+–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã):")
    lines.append(
        f"‚Ä¢ *–ü–ª–∞–Ω –ü—Ä–æ–¥–∞–∂–∏ (–ò—Ç–æ–≥–æ):* {overall.get('plan_total_sales', 0):.0f} –≥—Ä–Ω | *–§–∞–∫—Ç –ü—Ä–æ–¥–∞–∂–∏:* {overall.get('fact_sales', 0):.0f} –≥—Ä–Ω")
    lines.append(
        f"‚Ä¢ *–ü–ª–∞–Ω –ó–∞–∫–∞–∑–æ–≤ (—Å—É–º–º.):* {overall.get('plan_orders', 0):.0f} | *–§–∞–∫—Ç –ó–∞–∫–∞–∑–æ–≤:* {overall.get('fact_orders', 0):.0f}")
    lines.append(
        f"‚Ä¢ *–ü–ª–∞–Ω –°—Ä.–ó–∞–∫–∞–∑:* {overall.get('plan_avg_check', 0):.0f} –≥—Ä–Ω | *–§–∞–∫—Ç –°—Ä.–ó–∞–∫–∞–∑:* {overall.get('fact_avg_check', 0):.2f} –≥—Ä–Ω")
    lines.append(
        f"‚Ä¢ *–ü–ª–∞–Ω –ì–æ—Å—Ç–µ–π (–∑–∞–ª):* {overall.get('plan_guests', 0):.0f} | *–§–∞–∫—Ç –ì–æ—Å—Ç–µ–π:* {overall.get('fact_guests', 0):.0f}")
    final_text = "\n".join(lines)
    await send_long_message(context, update.effective_chat.id, final_text)


def main():
    os.makedirs(PLAN_FACT_FOLDER, exist_ok=True)
    app = ApplicationBuilder().token(BOT_TOKEN).build()

    conv_handler = ConversationHandler(
        entry_points=[CommandHandler("get_plan_fact", get_plan_fact_start)],
        states={
            GET_DATE: [MessageHandler(filters.TEXT & ~filters.COMMAND, get_date_handler)],
            CHOOSE_DEPARTMENT: [CallbackQueryHandler(choose_department_handler)]
        },
        fallbacks=[CommandHandler("cancel", cancel_handler)],
    )

    app.add_handler(CommandHandler("start", start_command))
    app.add_handler(CommandHandler("upload", upload_command))
    app.add_handler(MessageHandler(filters.Document.ALL, handle_document))
    app.add_handler(CommandHandler("test", test_command))
    app.add_handler(conv_handler)

    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞–≤—Ç–æ–æ—Ç—á—ë—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ 09:00)
    kiev_tz = pytz.timezone("Europe/Kiev")
    app.job_queue.run_daily(
        auto_report_job,
        time=time(hour=00, minute=6, second=0, tzinfo=kiev_tz),
        name="auto_report_job"
    )

    logging.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.")
    app.run_polling()


if __name__ == "__main__":
    main()
